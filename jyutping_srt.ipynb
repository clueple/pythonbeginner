{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09f7f2d3-8682-4b18-a084-b595bf97f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycantonese import characters_to_jyutping as ctj, segment\n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6957d330-2186-4ac9-84d8-d849fb3337a4",
   "metadata": {},
   "source": [
    "# **Backend functions for CantoneseTokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47fa040a-b911-4bd9-86b0-d2e0dbd6e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "can_pat = r'[\\u4e00-\\u9fff]+$' #pattern recognizing Chinese characters\n",
    "num_pat = r'(\\d)' #pattern that recognizing jyutping with the tone as the stop word \n",
    "rep_num_pat = r'\\1 '# replacing the num_pat with the tone and a space that follows\n",
    "\n",
    "def is_cantonese(str):\n",
    "    return re.match(can_pat, str)\n",
    "\n",
    "def split_jyut(str):\n",
    "    \"\"\" convert a Cantonese term's jyutping into a list of jyutping \n",
    "    e.g. 'gwong2dung1waa2'  -> ['gwong2','dung1','waa2']\n",
    "    \"\"\"\n",
    "    return re.sub(num_pat, rep_num_pat,str).split()\n",
    "\n",
    "def flat_list(nested):\n",
    "    \"\"\" flatten a nested list containing sublists \n",
    "    e.g. [['a','b'],['c','d'],['e']] -> ['a','b','c','d','e']\n",
    "    \"\"\"\n",
    "    #return [item for sublist in nested for item in sublist]\n",
    "    return [item for sublist in nested for item in (flat_list(sublist) if isinstance(sublist, list) else [sublist])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5bfb3e-15e2-44d2-80c5-8956daa78eb7",
   "metadata": {},
   "source": [
    "# **CantoneseTokenize Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4beff596-1fe5-4883-871d-12b4269a2025",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CantoneseTokenize:\n",
    "    def __init__(self, source_text):\n",
    "        # raw tokens from pycantonese.characters_to_jyutping\n",
    "        self.tokens = self.tokenize(source_text) \n",
    "        # derive the list of single Cantonese characters and English words & pronunciations from the raw tokens\n",
    "        self.word_tokens = flat_list([i[0] if i[1]==None else list(i[0]) for i in self.tokens])\n",
    "        # derive the list of jyutping or English for single Cantonese characters and English words & pronunciations from the raw tokens\n",
    "        self.jyut_tokens = flat_list([i[0] if i[1]==None else split_jyut(i[1]) for i in self.tokens])\n",
    "        # return the maximum width of the character in the two lists word_tokens and jyut_tokens\n",
    "        self.max_word_width = max([max(len(w),len(j)) for (w,j) in zip(self.word_tokens, self.jyut_tokens)])\n",
    "        # return the adjusted text for the word_tokens\n",
    "        #self.word_tokens_txt = '\\t'.join([i.center(self.max_word_width) for i in self.word_tokens])\n",
    "        self.word_tokens_txt = '\\t'.join([f\" {i.center(len(i))}\" for i in self.word_tokens])\n",
    "        # return the adjusted text for the jyut_tokens\n",
    "        #self.jyut_tokens_txt = '\\t'.join([i.center(self.max_word_width) for i in self.jyut_tokens])\n",
    "        self.jyut_tokens_txt = '\\t'.join([f\" {i.center(len((i)))}\" for i in self.jyut_tokens])\n",
    "        \n",
    "    def tokenize(self, source_text):\n",
    "        \"\"\" Tokenize the source_text using pycantonese \"\"\"\n",
    "        try: \n",
    "            tokens = ctj(source_text)\n",
    "            return tokens\n",
    "        except:\n",
    "            print(f\"error during tokenization {e}\")\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f226c9e-2e09-4826-aa34-29c7d779048c",
   "metadata": {},
   "source": [
    "# **Read srt file as a list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "330f5fb5-d4f3-475c-aa13-04514bb60999",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = r'g:/aud_cap'\n",
    "srt_file = r'edit2a_fhtml_part1_can_sub'\n",
    "file_ext = r'srt'\n",
    "\n",
    "src_input_file = f\"{file_dir}/{srt_file}.{file_ext}\"\n",
    "\n",
    "srt_src_list = Path(src_input_file).read_text(encoding='utf-8').split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6996f26-bfc9-4878-83fb-81dd88d6e274",
   "metadata": {},
   "source": [
    "# **Condition For Extracting Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd458d00-6f54-4247-965d-c01d6b7de274",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list_txt = '\\n'.join([f\"{CantoneseTokenize(i).word_tokens_txt}\\n{CantoneseTokenize(i).jyut_tokens_txt}\" if idx % 4 == 2 else i for idx, i in enumerate(srt_src_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd1f8560-cc5f-4089-bc57-0a89b89b91ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#print(output_list_txt[:700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d773762a-2f1d-4c5c-b26e-1dd0cae76365",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hoi1\t zo2\t player \n",
      "  開\t 咗\t player \n",
      " Length of r = 18 \n",
      " Length of w = 13\n"
     ]
    }
   ],
   "source": [
    "i_txt = '開咗player'\n",
    "r = CantoneseTokenize(i_txt).jyut_tokens_txt\n",
    "w = CantoneseTokenize(i_txt).word_tokens_txt\n",
    "print(r,'\\n',w,'\\n',f'Length of r = {len(r)}','\\n' ,f'Length of w = {len(w)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3d542ca-ed4c-48d4-bfa8-d84acbafd25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = r'jyutping_output'\n",
    "output_dir = r\"g:\"\n",
    "output_file = f\"{output_dir}/{output_name}.srt\"\n",
    "#Path(output_file).write_text(output_list_txt,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4383333e-18d6-4cb0-9f10-963cdaeff73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 13\n"
     ]
    }
   ],
   "source": [
    "print(len(r),len(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbf63b4-bf72-4887-87ff-bb43c75c6c41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
