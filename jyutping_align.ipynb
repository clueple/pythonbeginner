{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745ebaa5-5ece-4048-8918-351aa9d559c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycantonese import characters_to_jyutping as ctj, segment\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da956c69-96d8-41fc-93c6-c937fe584a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_txt = r'咁我就裝咗' #input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b136aa6c-784a-4dd1-8aa7-8ec1a501b6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "can_pat = r'[\\u4e00-\\u9fff]+$' #pattern recognizing Chinese characters\n",
    "num_pat = r'(\\d)' #pattern that recognizing jyutping with the tone as the stop word \n",
    "rep_num_pat = r'\\1 '# replacing the num_pat with the tone and a space that follows\n",
    "\n",
    "def is_cantonese(str):\n",
    "    return re.match(can_pat, str)\n",
    "\n",
    "def split_jyut(str):\n",
    "    \"\"\" convert a Cantonese term's jyutping into a list of jyutping \n",
    "    e.g. 'gwong2dung1waa2'  -> ['gwong2','dung1','waa2']\n",
    "    \"\"\"\n",
    "    return re.sub(num_pat, rep_num_pat,str).split()\n",
    "\n",
    "def flat_list(nested):\n",
    "    \"\"\" flatten a nested list containing sublists \n",
    "    e.g. [['a','b'],['c','d'],['e']] -> ['a','b','c','d','e']\n",
    "    \"\"\"\n",
    "    #return [item for sublist in nested for item in sublist]\n",
    "    return [item for sublist in nested for item in (flat_list(sublist) if isinstance(sublist, list) else [sublist])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0976f4b0-5f04-4639-90fb-fc622419fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CantoneseTokenize:\n",
    "    def __init__(self, source_text):\n",
    "        # raw tokens from pycantonese.characters_to_jyutping\n",
    "        self.tokens = self.tokenize(source_text) \n",
    "        # derive the list of single Cantonese characters and English words & pronunciations from the raw tokens\n",
    "        self.word_tokens = flat_list([i[0] if i[1]==None else list(i[0]) for i in self.tokens])\n",
    "        # derive the list of jyutping or English for single Cantonese characters and English words & pronunciations from the raw tokens\n",
    "        self.jyut_tokens = flat_list([i[0] if i[1]==None else split_jyut(i[1]) for i in self.tokens])\n",
    "        # return the maximum width of the character in the two lists word_tokens and jyut_tokens\n",
    "        self.max_word_width = max([max(len(w),len(j)) for (w,j) in zip(self.word_tokens, self.jyut_tokens)])\n",
    "        # return the adjusted text for the word_tokens\n",
    "        self.word_tokens_txt = '\\t'.join([i.center(self.max_word_width) for i in self.word_tokens])\n",
    "        # return the adjusted text for the jyut_tokens\n",
    "        self.jyut_tokens_txt = '\\t'.join([i.center(self.max_word_width) for i in self.jyut_tokens])\n",
    "        \n",
    "    def tokenize(self, source_text):\n",
    "        \"\"\" Tokenize the source_text using pycantonese \"\"\"\n",
    "        try: \n",
    "            tokens = ctj(source_text)\n",
    "            return tokens\n",
    "        except:\n",
    "            print(f\"error during tokenization {e}\")\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6b0445-5031-47fc-9049-90ad33152e98",
   "metadata": {},
   "source": [
    "# **preview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a090763b-9438-4ef4-b253-569c9a085396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ths is the tokens:\n",
      "[('咁', 'gam3'), ('我', 'ngo5'), ('就', 'zau6'), ('裝', 'zong1'), ('咗', 'zo2')]\n",
      "\n",
      "This is the word_tokens:\n",
      "['咁', '我', '就', '裝', '咗']\n",
      "\n",
      "This is the jyut_tokens:\n",
      "['gam3', 'ngo5', 'zau6', 'zong1', 'zo2']\n",
      "\n",
      "The index 3 is 裝\n",
      "The length of the longest word is 5\n",
      "This is the word tokens txt:\n",
      "  咁  \t  我  \t  就  \t  裝  \t  咗  \n",
      "This is the jyut tokens txt:\n",
      " gam3\t ngo5\t zau6\tzong1\t zo2 \n",
      "This is the output text:\n",
      "  咁  \t  我  \t  就  \t  裝  \t  咗  \n",
      " gam3\t ngo5\t zau6\tzong1\t zo2 \n"
     ]
    }
   ],
   "source": [
    "sent_tok = CantoneseTokenize(source_text=src_txt)\n",
    "tokens = sent_tok.tokens\n",
    "jyut_tokens = sent_tok.jyut_tokens\n",
    "word_tokens = sent_tok.word_tokens\n",
    "\n",
    "print(f\"\"\"Ths is the tokens:\\n{tokens}\\n\n",
    "This is the word_tokens:\\n{word_tokens}\\n\n",
    "This is the jyut_tokens:\\n{jyut_tokens}\n",
    "\"\"\")\n",
    "print(f\"The index 3 is {word_tokens[3]}\")\n",
    "print(f\"The length of the longest word is {sent_tok.max_word_width}\")\n",
    "print(f\"This is the word tokens txt:\\n{sent_tok.word_tokens_txt}\")\n",
    "print(f\"This is the jyut tokens txt:\\n{sent_tok.jyut_tokens_txt}\")\n",
    "print(f\"This is the output text:\\n{sent_tok.word_tokens_txt}\\n{sent_tok.jyut_tokens_txt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4999c5-bec8-4242-a523-8478126f7f6c",
   "metadata": {},
   "source": [
    "# **Explaining how to flatten a list of sublist with for-loops**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55903a91-54cb-49cd-9c67-fdb93aefa6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
