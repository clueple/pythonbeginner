{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745ebaa5-5ece-4048-8918-351aa9d559c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycantonese import characters_to_jyutping as ctj, segment\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fd0abe8-402f-4da2-8193-58112b7232e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_txt = r'你都short short哋嘅。' #input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b136aa6c-784a-4dd1-8aa7-8ec1a501b6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "can_pat = r'[\\u4e00-\\u9fff]+$' #pattern recognizing Chinese characters\n",
    "num_pat = r'(\\d)' #pattern that recognizing jyutping with the tone as the stop word \n",
    "rep_num_pat = r'\\1 '# replacing the num_pat with the tone and a space that follows\n",
    "\n",
    "def is_cantonese(str):\n",
    "    \"\"\" check whether a text string is Cantonese \"\"\"\n",
    "    return re.match(can_pat, str)\n",
    "\n",
    "def split_jyut(str):\n",
    "    \"\"\" convert a Cantonese term's jyutping into a list of jyutping \n",
    "    e.g. 'gwong2dung1waa2'  -> ['gwong2','dung1','waa2']\n",
    "    \"\"\"\n",
    "    return re.sub(num_pat, rep_num_pat,str).split()\n",
    "\n",
    "def flat_list(nested):\n",
    "    \"\"\" flatten a nested list containing sublists \n",
    "    e.g. [['a','b'],['c','d'],['e']] -> ['a','b','c','d','e']\n",
    "    \"\"\"\n",
    "    #return [item for sublist in nested for item in sublist]\n",
    "    return [item for sublist in nested for item in (sublist if isinstance(sublist, list) else [sublist])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0976f4b0-5f04-4639-90fb-fc622419fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CantoneseTokenize:\n",
    "    def __init__(self, source_text):\n",
    "        # raw tokens from pycantonese.characters_to_jyutping\n",
    "        self.tokens = self.tokenize(source_text) \n",
    "        # derive the list of single Cantonese characters and English words & pronunciations from the raw tokens\n",
    "        self.word_tokens = flat_list([i[0] if i[1]==None else list(i[0]) for i in self.tokens])\n",
    "        # derive the list of jyutping or English for single Cantonese characters and English words & pronunciations from the raw tokens\n",
    "        self.jyut_tokens = flat_list([i[0] if i[1]==None else split_jyut(i[1]) for i in self.tokens])\n",
    "        \n",
    "    def tokenize(self, source_text):\n",
    "        \"\"\" Tokenize the source_text using pycantonese \"\"\"\n",
    "        try: \n",
    "            tokens = ctj(source_text)\n",
    "            return tokens\n",
    "        except:\n",
    "            print(f\"error during tokenization {e}\")\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6b0445-5031-47fc-9049-90ad33152e98",
   "metadata": {},
   "source": [
    "# **preview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a090763b-9438-4ef4-b253-569c9a085396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ths is the tokens:\n",
      "[('你', 'nei5'), ('都', 'dou1'), ('shortshort', None), ('哋', 'dei2'), ('嘅', 'ge3'), ('。', None)]\n",
      "\n",
      "This is the word_tokens:\n",
      "['你', '都', 'shortshort', '哋', '嘅', '。']\n",
      "\n",
      "This is the jyut_tokens:\n",
      "['nei5', 'dou1', 'shortshort', 'dei2', 'ge3', '。']\n",
      "\n",
      "The index 3 is 哋\n"
     ]
    }
   ],
   "source": [
    "sent_tok = CantoneseTokenize(source_text=src_txt)\n",
    "tokens = sent_tok.tokens\n",
    "jyut_tokens = sent_tok.jyut_tokens\n",
    "word_tokens = sent_tok.word_tokens\n",
    "\n",
    "print(f\"\"\"Ths is the tokens:\\n{tokens}\\n\n",
    "This is the word_tokens:\\n{word_tokens}\\n\n",
    "This is the jyut_tokens:\\n{jyut_tokens}\n",
    "\"\"\")\n",
    "print(f\"The index 3 is {word_tokens[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4999c5-bec8-4242-a523-8478126f7f6c",
   "metadata": {},
   "source": [
    "# **Test Styling For WordTokens & JyutTokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b6b00fe-84b9-4643-ba3a-3dcba4a04aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Return the number of characters for the longest word or jyutping\"\"\"\n",
    "width = max([max(len(x),len(y)) for (x,y) in zip(word_tokens, jyut_tokens)])\n",
    "width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8cb4eed-504e-4a91-976d-3df2bcbb5156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    你         都     shortshort    哋         嘅         。     \n",
      "   nei5       dou1    shortshort   dei2       ge3         。     \n"
     ]
    }
   ],
   "source": [
    "w_txt = ''.join([i.center(width) if is_cantonese(i) else i.center(width)  for i in word_tokens])\n",
    "j_txt = ''.join([i.center(width+1) if is_cantonese(word_tokens[idx-1]) else i.center(width) for idx, i in enumerate(jyut_tokens)])\n",
    "\n",
    "o_txt = f\"{w_txt}\\n{j_txt}\"\n",
    "print(o_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e10953-531b-49c7-adcc-b3d3a1fe0b16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
